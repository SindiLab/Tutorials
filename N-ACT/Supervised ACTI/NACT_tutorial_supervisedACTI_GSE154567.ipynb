{
  "metadata": {
    "kernelspec": {
      "display_name": "NACT Kernel",
      "language": "python",
      "name": "nact"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "nbproject": {
      "id": "OU1PGCOxUshr",
      "version": "0",
      "time_init": "2023-01-27T00:13:40.293369+00:00",
      "pypackage": null,
      "parent": null,
      "user_handle": null,
      "user_id": null,
      "user_name": null
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# N-ACT Tutorial: Evaluating NACT's Supervised Automatic Cell Type Identifcation (ACTI)\n",
        "\n",
        "In this notebook, we will go over how to load a pre-trained NACT model for classifying pre-labeled cell types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table><tbody><tr><td style='text-align: left;'><b>id</b></td><td style='text-align: left;'>OU1P<span style='opacity:0.3'>GCOxUshr</span></td></tr><tr><td style='text-align: left;'><b>version</b></td><td style='text-align: left;'>0</td></tr><tr><td style='text-align: left;'><b>time_init</b></td><td style='text-align: left;'>2023-01-27 00:13</td></tr><tr><td style='text-align: left;'><b>time_run</b></td><td style='text-align: left;'>2023-01-27 00:13</td></tr><tr><td style='text-align: left;'><b>pypackage</b></td><td style='text-align: left;'>nact==0.1.0 nbproject==0.8.1 numpy==1.23.5 pandas==1.5.2 scanpy==1.9.1 torch==1.13.1</td></tr></tbody></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from nbproject import header\n",
        "    header(filepath=\"/home/aheydari/SindiLabTutorials/N-ACT/Supervised ACTI/\"\n",
        "           \"NACT_tutorial_supervisedACTI_GSE154567.ipynb\")\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Please install nbproject (pip install nbproject) to see header\"\n",
        "         \"dependencies.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nact\n",
        "from nact.utilities import *\n",
        "from nact import scanpy_to_dataloader\n",
        "from nact import AttentionQuery\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NACT Version: 0.1.0\n",
            "Scanpy Version: 1.9.1\n"
          ]
        }
      ],
      "source": [
        "print(f\"NACT Version: {nact.__version__}\")\n",
        "print(f\"Scanpy Version: {sc.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Up Result Folder and Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "abs_path = \"/home/aheydari/\"\n",
        "local_path = \"data/NACT_Data/Supervised Benchmarking/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# label for the dataset folder we want to make\n",
        "dataset_name = \"GSE154567\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load in pre-trained NACT model\n",
        "\n",
        "Since our implementation is in pytorch, we can use the `load` funtion that pytorch provides. Our model is stored as a dict, with `epoch` corresponding to the current epoch, and `Saved_Model` corresponding to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "## Load in the data that we want\n",
        "\n",
        "For example, here we will load in cluster 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_to_data = (f\"{abs_path}{local_path}{dataset_name}\"\n",
        "                \"_qc_hvg_anno_5k_raw_train_split.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Reading in Scanpy/Seurat AnnData\n",
            "    -> Trying adata.raw.X instead of adata.X!\n",
            "    -> Splitting Train and Validation Data\n",
            "==> Using cluster info for generating train and validation labels\n",
            "==> Checking if we have sparse matrix into dense\n"
          ]
        }
      ],
      "source": [
        "train_data_loader, test_data_loader = scanpy_to_dataloader(path_to_data,\n",
        "                                                test_no_valid = True, \n",
        "                                                verbose = False, \n",
        "                                                raw_x = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NACTProjectionAttention(\n",
              "  (masking_layer): Identity()\n",
              "  (attention_module): Linear(in_features=5000, out_features=5000, bias=True)\n",
              "  (projection_block1): Projection(\n",
              "    (projection): Linear(in_features=5000, out_features=5000, bias=True)\n",
              "    (output_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (normalization): LayerNorm((5000,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (projection_block2): Projection(\n",
              "    (projection): Linear(in_features=5000, out_features=5000, bias=True)\n",
              "    (output_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (normalization): LayerNorm((5000,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (pwff): PointWiseFeedForward(\n",
              "    (first_layer): Sequential(\n",
              "      (0): Linear(in_features=5000, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (second_layer): Linear(in_features=128, out_features=5000, bias=True)\n",
              "    (normalization): LayerNorm((5000,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (task_module): Sequential(\n",
              "    (0): Linear(in_features=5000, out_features=9, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dict = torch.load(f\"{abs_path}data/NACT_Trained_Models/\"\n",
        "                              \"NACT_Jan2023Benchmarks/NACT-Pojections+Attention\"\n",
        "                              f\"-{dataset_name}.pth\",\n",
        "                              map_location=torch.device('cpu'))\n",
        "\n",
        "trained_nact_model = model_dict[\"Saved_Model\"]\n",
        "trained_nact_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Using GPU (CUDA)\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = \"cuda\";\n",
        "        print('==> Using GPU (CUDA)')\n",
        "        \n",
        "elif(torch.backends.mps.is_available()):\n",
        "    device = torch.device(\"mps\");\n",
        "    print('==> Using M1 GPUs')\n",
        "\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print('==> Using CPU')\n",
        "    print('    -> Warning: Using CPUs will yield to slower training time than GPUs')\n",
        "    \n",
        "trained_nact_model.device = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking the Accuracy of ACTI on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Evaluating on Validation Set:\n",
            "    -> Accuracy of classifier network on validation set:92.6698 %\n",
            "    -> Non-Weighted F1 Score on validation set: 0.8986\n",
            "    -> Weighted F1 Score on validation set: 0.9265\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.98      0.98      1862\n",
            "         1.0       0.94      0.95      0.95      5103\n",
            "         2.0       0.86      0.88      0.87      1812\n",
            "         3.0       0.93      0.87      0.90       800\n",
            "         4.0       0.94      0.91      0.93      1096\n",
            "         5.0       0.97      0.97      0.97      1238\n",
            "         6.0       0.93      0.89      0.91        63\n",
            "         7.0       0.89      0.70      0.78        79\n",
            "         8.0       0.82      0.80      0.81       907\n",
            "\n",
            "    accuracy                           0.93     12960\n",
            "   macro avg       0.92      0.88      0.90     12960\n",
            "weighted avg       0.93      0.93      0.93     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This utility function outputs arrays related to classification, but since we\n",
        "# do not need them we ignore them (i.e. use `_` as the variable)\n",
        "\n",
        "_, _ , _ , _ ,  _ = evaluate_classifier(test_data_loader, \n",
        "                    trained_nact_model, \n",
        "                    classification_report=True,\n",
        "                    device=trained_nact_model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}